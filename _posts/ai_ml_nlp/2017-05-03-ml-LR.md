---
layout: post
title: 逻辑回归模型基础
category: ML
comments: false
---
# 1. 概述
逻辑回归(Logistic Regression, LR)模型其实仅在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数，使得逻辑回归模型成为了机器学习领域一颗耀眼的明星，更是计算广告学的核心。

首先要搞清楚当你的目标变量是分类变量时，才会考虑逻辑回归，并且主要用于两分类问题。

**概率 和 似然**  
但是在统计学中，二者有截然不同的用法。概率描述了已知参数时的随机变量的输出结果；似然则用来描述已知随机变量输出结果时，未知参数的可能取值。例如，对于“一枚正反对称的硬币上抛十次”这种事件，我们可以问硬币落地时十次都是正面向上的“概率”是多少；而对于“一枚硬币上抛十次，落地都是正面向上”这种事件，我们则可以问，这枚硬币正反面对称的“似然”程度是多少。

由于对数函数是单调递增的，而且对数似然函数在极大化求解时较为方便，所以对数似然函数常用在最大似然估计及相关领域中。

# 2. 介绍

## 2.1 逻辑回归
最简单的回归是线性回归。但是线性回归的鲁棒性很差，因为线性回归在整个实数域内敏感度一致，而分类范围，需要在[0,1] （二分类 ）.

线性回归是为了用一个函数来划分一堆数据，函数的接近与否，与参数有关系（其实就是为了求参数）。于是有『损失函数』来衡量在某个参数下逻辑函数的准确程度。求解损失函数过程，是一个解最优化的问题的过程，可以用最小二乘法，梯度下降法。

逻辑回归的模型 是一个非线性模型，sigmoid函数，又称逻辑回归函数。但是它本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归的。可以说，逻辑回归，都是以线性回归为理论支持的。

只不过，线性模型，无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

逻辑回归是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型。

逻辑方程和逻辑曲线：

![lr function](/images/201705/lrFunction.gif)

（x的输入可以是负无穷到正无穷，但是y的输出总是[0,1]. x=0时，y=0.5，这是决策边界——能够分开这两类样本的边界。）

对于多元逻辑回归，可用如下公式似合分类，其中公式(4)的变换，将在逻辑回归模型参数估计时，化简公式带来很多益处，y={0,1}为分类结果。 

![lr2](/images/201705/lr2.gif)

对于训练数据集，特征数据x={x 1 , x 2 , … , x m }和对应的分类数据y={y 1 , y 2 , … , y m }。构建逻辑回归模型f(θ)，最典型的构建方法便是应用极大似然估计。首先，对于单个样本，其后验概率为：

![lr3](/images/201705/lr3.gif)

那么，极大似然函数为：

![lr4](/images/201705/lr4.gif)

 log似然是：

![lr5](/images/201705/lr5.gif)

## 2.2 梯度下降

由第1节可知，求逻辑回归模型f(θ)，等价于：

![lr6](/images/201705/lr6.gif)    

采用梯度下降法：

![lr7](/images/201705/lr7.gif)     

从而迭代θ至收敛即可：

![lr8](/images/201705/lr8.gif)

## 2.3 模型评估

对于LR分类模型的评估，常用AUC来评估，关于AUC的更多定义与介绍，可见参考文献2，在此只介绍一种极简单的计算与理解方法。

对于下图的分类：

![lr9](/images/201705/lr9.gif)

对于训练集的分类，训练方法1和训练方法2分类正确率都为80%，但明显可以感觉到训练方法1要比训练方法2好。因为训练方法1中，5和6两数据分类错误，但这两个数据位于分类面附近，而训练方法2中，将10和1两个数据分类错误，但这两个数据均离分类面较远。

AUC正是衡量分类正确度的方法，将训练集中的label看两类{0，1}的分类问题，分类目标是将预测结果尽量将两者分开。将每个0和1看成一个pair关系，团中的训练集共有5*5=25个pair关系，只有将所有pair关系一至时，分类结果才是最好的，而auc为1。在训练方法1中，与10相关的pair关系完全正确，同样9、8、7的pair关系也完全正确，但对于6，其pair关系(6，5)关系错误，而与4、3、2、1的关系正确，故其auc为(25-1)/25=0.96；对于分类方法2，其6、7、8、9的pair关系，均有一个错误，即(6,1)、(7,1)、(8,1)、(9,1)，对于数据点10，其正任何数据点的pair关系，都错误，即(10,1)、(10,2)、(10,3)、(10,4)、(10,5)，故方法2的auc为(25-4-5)/25=0.64，因而正如直观所见，分类方法1要优于分类方法2。

# 2.4 演算手稿

附演算手稿如下：

![lr10](/images/201705/lr10.gif)

## 3. 逻辑回归估计

损失函数是在机器学习中最常出现的概念，用于衡量均方误差[（模型估计值－模型实际值）^2／ n]最小，即预测的准确性，因而需要损失函数最小，得到的参数才最优。（线性回归中的最小二乘估计也是由此而来）但因为逻辑回归的这种损失函数非凸，不能找到全局最低点。因此，需要采用另一种方式，将其转化为求最大似然，如下，具体的推导求解过程可参见博客 http://blog.csdn.net/suipingsp/article/details/41822313


## 4. 正则化 Regularization

## 4.1 过拟合问题
对于线性回归或逻辑回归的损失函数构成的模型，可能会有些权重很大，有些权重很小，导致过拟合（就是过分拟合了训练数据），使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。

下面左图即为欠拟合，中图为合适的拟合，右图为过拟合。

![nihe](/images/201705/nihe0.png)

** 问题的主因 **

过拟合问题往往源自过多的特征。

** 解决方法 **

1）减少特征数量（减少特征会失去一些信息，即使特征选的很好）
- 可用人工选择要保留的特征；
- 模型选择算法；

2）正则化（特征较多时比较有效）
- 保留所有特征，但减少θ的大小

** 正则化方法 **
正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。

# 参考
> - [http://www.cnblogs.com/sparkwen/p/3441197.html?utm_source=tuicool&utm_medium=referral](http://www.cnblogs.com/sparkwen/p/3441197.html?utm_source=tuicool&utm_medium=referral)


 
