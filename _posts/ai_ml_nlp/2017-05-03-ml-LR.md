---
layout: post
title: 逻辑回归模型基础
category: ML
comments: false
---
# 1. 概述
逻辑回归(Logistic Regression, LR)模型其实仅在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数，使得逻辑回归模型成为了机器学习领域一颗耀眼的明星，更是计算广告学的核心。

# 2. 介绍

## 2.1 逻辑回归
最简单的回归是线性回归。但是线性回归的鲁棒性很差，因为线性回归在整个实数域内敏感度一致，而分类范围，需要在[0,1].

逻辑回归是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型。

逻辑方程和逻辑曲线：
![lr function](/images/201705/lrFunction.gif)

对于多元逻辑回归，可用如下公式似合分类，其中公式(4)的变换，将在逻辑回归模型参数估计时，化简公式带来很多益处，y={0,1}为分类结果。 

![lr2](/images/201705/lr2.gif)

对于训练数据集，特征数据x={x 1 , x 2 , … , x m }和对应的分类数据y={y 1 , y 2 , … , y m }。构建逻辑回归模型f(θ)，最典型的构建方法便是应用极大似然估计。首先，对于单个样本，其后验概率为：
[lr3](/images/201705/lr3.gif)

那么，极大似然函数为：
[lr4](/images/201705/lr4.gif)

 log似然是：
[lr5](/images/201705/lr5.gif)

## 2.2 梯度下降

由第1节可知，求逻辑回归模型f(θ)，等价于：

[lr6](/images/201705/lr6.gif)    采用梯度下降法：

[lr7](/images/201705/lr7.gif)     从而迭代θ至收敛即可：

[lr8](/images/201705/lr8.gif)

## 2.3 模型评估

对于LR分类模型的评估，常用AUC来评估，关于AUC的更多定义与介绍，可见参考文献2，在此只介绍一种极简单的计算与理解方法。

对于下图的分类：

[lr9](/images/201705/lr9.gif)

   对于训练集的分类，训练方法1和训练方法2分类正确率都为80%，但明显可以感觉到训练方法1要比训练方法2好。因为训练方法1中，5和6两数据分类错误，但这两个数据位于分类面附近，而训练方法2中，将10和1两个数据分类错误，但这两个数据均离分类面较远。

AUC正是衡量分类正确度的方法，将训练集中的label看两类{0，1}的分类问题，分类目标是将预测结果尽量将两者分开。将每个0和1看成一个pair关系，团中的训练集共有5*5=25个pair关系，只有将所有pair关系一至时，分类结果才是最好的，而auc为1。在训练方法1中，与10相关的pair关系完全正确，同样9、8、7的pair关系也完全正确，但对于6，其pair关系(6，5)关系错误，而与4、3、2、1的关系正确，故其auc为(25-1)/25=0.96；对于分类方法2，其6、7、8、9的pair关系，均有一个错误，即(6,1)、(7,1)、(8,1)、(9,1)，对于数据点10，其正任何数据点的pair关系，都错误，即(10,1)、(10,2)、(10,3)、(10,4)、(10,5)，故方法2的auc为(25-4-5)/25=0.64，因而正如直观所见，分类方法1要优于分类方法2。

# 2.4 演算手稿

附演算手稿如下：

[lr10](/images/201705/lr10.gif)

 
