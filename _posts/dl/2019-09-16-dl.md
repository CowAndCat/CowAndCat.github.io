---
layout: post
title: 《动手学习深度学习》
category: DL
comments: false
---

访问https://zh.d2l.ai，获取本书的最新版本或正式版本。

# 一、写在前面

本书的代码基于Apache MXNet实现。MXNet是一个开源的深度学习框架。它是AWS(亚⻢逊云 计算服务)首选的深度学习框架，也被众多学校和公司使用。

目前的机器学习和深度学习应用共同的核心思想:我们可以 称其为“用数据编程”。

深度学习框架也在传播深度学习思想的过程中扮演了重要⻆色。Caffe、Torch和Theano这 样的第一代框架使建模变得更简单。许多开创性的论文都用到了这些框架。如今它们已经 被TensorFlow(经常是以高层API Keras的形式被使用)、CNTK、Caffe2 和Apache MXNet所 取代。第三代，即命令式深度学习框架，是由用类似NumPy的语法来定义模型的 Chainer所 开创的。这样的思想后来被 PyTorch和MXNet的Gluon API 采用，后者也正是本书用来教学 深度学习的工具。

深度学习的应用
- 字体识别
- 物体识别（皮肤癌诊断、鸟类识别、猫狗识别等）
- 语言识别
- 对话（苹果的siri，亚⻢逊的Alexa和谷歌助手等）
- 博弈（下棋、星际争霸游戏等）
- 自动驾驶
- 自然语言处理

# 二、安装
## 2.1 启动环境

```
$ conda activate gluon
$ jupyter notebook

$ conda deactivate
```

## 2.2 尝试jupyter notebook
能写一行代码出一次结果，体验不错。

NDArray是MXNet中存储和变换数据的主要工具。

## 2.3 自动求梯度 (未完)
hold，这里有些基础看不懂，跳到最后看看『数学基础』。

# 三、数学基础
## 3.1 运算
向量a和b的点乘（内积）是一个标量。

矩阵乘法不同，每个元素是原位置的行\*列之和。一个m\*p的矩阵和p\*n的矩阵相乘，得到一个m\*n的矩阵。

## 3.2 范数
向量的范数是各元素的绝对值的p次方之和的根号p的值。

例如，向量x的L1 范数是该向量元素绝对值之和。
向量x的L2 范数是该向量元素平方和的平方根。

我们通常用∥x∥指代∥x∥2 。

矩阵X的Frobenius范数为该矩阵元素平方和的平方根。

## 3.3 微分 && 偏导数
（看书上的公式）

## 3.4 梯度 && 海森矩阵
看不懂。。
为表示简洁，我们有时用∇f(x)代替∇xf(x)。

## 3.5 和数学不相关的基础
NVIDIA有面向个人用戶(如GTX系列)和企业用戶(如Tesla系列)的两类GPU。这两类GPU的 计算能力相当。然而，面向企业用戶的GPU通常使用被动散热并增加了显存校验，从而更适合数 据中心，并通常要比面向个人用戶的GPU贵上10倍。

GPU的性能主要由以下3个参数构成。

1. 计算能力。通常我们关心的是32位浮点计算能力。16位浮点训练也开始流行，如果只做预测的话也可以用8位整数。
2. 显存大小。当模型越大或者训练时的批量越大时，所需要的显存就越多。 
3. 显存带宽。只有当显存带宽足够时才能充分发挥计算能力。

## 2.3 自动求梯度（继续）
attach_grad函数用来申请存储梯度所需要的内存。

# 四、深度学习基础

作为机器学习的一类，深度学习通常基于神经网络模型逐级表示越来越抽象的概念或模式。

先介绍单层神经网络:线性回归和softmax回归。

## 4.1 线性回归

线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常⻅，如预测房屋价 格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。 我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适用于分类问题。


### 4.1.1 线性回归的基本要素

模型：从自变量得到因变量的函数，类似于 y=x1w1 + x2w2 + b

模型训练：通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这一过程叫做训练。

模型训练一般涉及三个要素：训练数据、损失函数、优化算法。

损失函数(loss function)：在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为 误差，且数值越小表示误差越小。一个常用的选择是平方函数(square loss)。

在模型训练中，我们希望找出一组模型参数，来使训练样本平均损失最小。

优化算法：
当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解(analytical solution)。本节使用的线性回归和平方误差刚好属于这个范畴。然而， 大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解(numerical solution)

我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。

### 4.1.2 线性回归的表示方法

线性回归是一个单层神经网络。可以分为输入层和输出层，由于输入层并不涉及计算，不算入神经网络的层数。

输出层中的神经元和输入 层中各个输入完全连接。因此，这里的输出层又叫全连接层(fully-connected layer)或稠密层
(dense layer)。