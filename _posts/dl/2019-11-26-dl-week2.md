---
layout: post
title: 《动手学习深度学习》
category: DL
comments: false
---

# 21天系列 (week2 11月25日 至 11月29日）#

在北京，看书时间得用力挤出来。

## 3.8 多层感知机（multilayer perceptron，MLP）

多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。多层感知机中的隐藏层和输出层都是全连接层。

虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络。

多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。

### 3.8.2 激活函数
上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。

#### 3.8.2.1 ReLU函数
ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。只保留正数元素，并将负数元素清零。

一种实现：

    def relu(X):
        return nd.maximum(X, 0)

#### 3.8.2.2 sigmoid函数
sigmoid函数可以将元素的值变换到0和1之间.

sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。

#### 3.8.2.3 tanh函数
tanh（双曲正切）函数可以将元素的值变换到-1和1之间

## 3.11 模型选择、欠拟合和过拟合

### 3.11.1 训练误差和泛化误差
在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。

让我们以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。假设训练题和测试题都随机采样于一个未知的依照相同考纲的巨大试题库。如果让一名未学习中学知识的小学生去答题，那么测试题和训练题的答题错误率可能很相近。但如果换成一名反复练习训练题的高三备考生答题，即使在训练题上做到了错误率为0，也不代表真实的高考成绩会如此。

### 3.11.2 欠拟合和过拟合
接下来，我们将探究模型训练中经常出现的两类典型问题：一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。

# REF
>[sigmoid和tanh求导的最终结果,以及Sigmoid函数与损失函数求导](https://blog.csdn.net/hhtnan/article/details/78316785)  