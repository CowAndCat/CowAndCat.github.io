---
layout: post
title: 《动手学习深度学习》
category: DL
comments: false
---

# 21天系列 (week2 11月25日 至 11月29日）#

在北京，看书时间得用力挤出来。

## 3.8 多层感知机（multilayer perceptron，MLP）

多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。多层感知机中的隐藏层和输出层都是全连接层。

虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络。

多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。

### 3.8.2 激活函数
上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。

#### 3.8.2.1 ReLU函数
ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。只保留正数元素，并将负数元素清零。

一种实现：

    def relu(X):
        return nd.maximum(X, 0)

#### 3.8.2.2 sigmoid函数
sigmoid函数可以将元素的值变换到0和1之间.

sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。

#### 3.8.2.3 tanh函数
tanh（双曲正切）函数可以将元素的值变换到-1和1之间

## 3.11 模型选择、欠拟合和过拟合

### 3.11.1 训练误差和泛化误差
在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。

让我们以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。假设训练题和测试题都随机采样于一个未知的依照相同考纲的巨大试题库。如果让一名未学习中学知识的小学生去答题，那么测试题和训练题的答题错误率可能很相近。但如果换成一名反复练习训练题的高三备考生答题，即使在训练题上做到了错误率为0，也不代表真实的高考成绩会如此。

机器学习模型应关注泛化误差。

### 3.11.2 欠拟合和过拟合
接下来，我们将探究模型训练中经常出现的两类典型问题：一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。

### 3.11.3 应对过拟合的方法
权重衰减、丢弃法、增大训练量以及使用复杂度合适的模型,能用于应对过拟合情况。

其中权重衰减是为惩罚函数添加惩罚项使得学出的模型参数较小。

丢弃法会处理隐藏层的结果，只在模型训练的时候使用。

### 3.14 正向传播、反向传播和计算图

正向传播是指对神经网络沿着从输入层到输出层的顺序,依次计算并存储模型的中间变量(包括
输出)

我们通常绘制计算图来可视化运算符和变量在计算中的依赖关系。

反向传播指的是计算神经网络参数梯度的方法。总的来说,反向传播依据微积分中的链式法则, 沿着从输出层到输入层的顺序,依次计算并存储目标函数有关神经网络各层的中间变量以及参数 的梯度。

在模型参数初始化完成后,我们交替地进行正向传播和反向传播,并根据反向传播计算的 梯度迭代模型参数。既然我们在反向传播中使用了正向传播中计算得到的中间变量来避免重复计 算,那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用 更多内存的一个重要原因。另外需要指出的是,这些中间变量的个数大体上与网络层数线性相关, 每个变量的大小跟批量大小和输入个数也是线性相关的,它们是导致较深的神经网络使用较大批 量训练时更容易超内存的主要原因。

反向传播沿着从输出层到输入层的顺序,依次计算并存储神经网络中间变量和参数的梯度。

在训练深度学习模型时,正向传播和反向传播相互依赖。

### 3.15. 数值稳定性和模型初始化
理解了正向传播与反向传播以后，我们来讨论一下深度学习模型的数值稳定性问题以及模型参数的初始化方法。深度模型有关数值稳定性的典型问题是衰减（vanishing）和爆炸（explosion）。

当神经网络的层数较多时，模型的数值稳定性容易变差。

举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入 X 分别与 0.2^30≈1×10^−21 （衰减）和 5^30≈9×10^20 （爆炸）的乘积。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。

#### 3.15.2. 随机初始化模型参数
在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。

回顾“多层感知机”一节图3.3描述的多层感知机。为了方便解释，假设输出层只保留一个输出单元 o1 （删去 o2 和 o3 以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。

在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。

# REF
>[sigmoid和tanh求导的最终结果,以及Sigmoid函数与损失函数求导](https://blog.csdn.net/hhtnan/article/details/78316785)   